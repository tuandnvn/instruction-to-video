{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, LSTM, Dense, RepeatVector\n",
    "from numpy import array, array_equal, argmax\n",
    "import tensorflow as tf\n",
    "import attention_decoder\n",
    "\n",
    "\"\"\"\n",
    "Sample keras model\n",
    "\"\"\"\n",
    "def create_model_1():\n",
    "    Note = Input(shape=(sequence_length, n_vocab), dtype = 'float32')\n",
    "    y = LSTM(400, input_shape = (sequence_length, n_vocab), return_sequences = True ) (Note)\n",
    "    y = Dropout(0.4) (y)\n",
    "    y = LSTM(400) (y)\n",
    "    y = Dropout(0.4) (y)\n",
    "    # Two weights, two bias\n",
    "    # If coordinates of input is (X1, X2), and this layer is (Y1, Y2, Y3, Y4)\n",
    "    # Result would be \n",
    "    y1 = Lambda(lambda x: x*2)( Dense(2, activation = 'tanh') (y) )\n",
    "    y2 = Dense(2, activation = 'linear') (y)\n",
    "    Coordinates = Input(shape= (2, ), dtype = 'float32')\n",
    "    print (keras.backend.shape(y1))\n",
    "    print (keras.backend.shape(Coordinates))\n",
    "    c1 = Multiply()([y1, Coordinates])\n",
    "    c = Add()([y2, c1])\n",
    "    m = Model(inputs = [Note, Coordinates], outputs = c)\n",
    "\n",
    "    print (m.summary())\n",
    "    m.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return m\n",
    "\n",
    "def create_sample_attention_model () :\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "    model.add(attention_decoder.AttentionDecoder(150, n_features))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like for this problem, I need to use Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_ta = tf.TensorArray(size=784, dtype=tf.float32)  # store trained/sampled pixel\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None for time == 0\n",
    "    if cell_output is None:\n",
    "        # time=0, everything here will be used for initialization only\n",
    "        next_cell_state = cell_init_state\n",
    "        next_pixel = cell_init_pixel\n",
    "        next_loop_state = output_ta\n",
    "    else:  \n",
    "        # pass the last state to the next\n",
    "        next_cell_state = cell_state\n",
    "        next_pixel = tf.cond(is_training,\n",
    "                         lambda: inputs_ta.read(time - 1),\n",
    "                         lambda: tf.contrib.distributions.Bernoulli(\n",
    "                             probs=tf.nn.sigmoid(tf.layers.dense(cell_output, 1, \n",
    "                                 name='output_to_p', activation=tf.nn.sigmoid,\n",
    "                                 reuse=True)),\n",
    "                             dtype=tf.float32).sample())\n",
    "        next_loop_state = loop_state.write(time - 1, next_pixel)\n",
    "    elements_finished = (time >= 784)\n",
    "    return (elements_finished, next_pixel, next_cell_state,\n",
    "            emit_output, next_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "\n",
    "def sampling_rnn(self, cell, initial_state, input_, seq_lengths):\n",
    "\n",
    "    # raw_rnn expects time major inputs as TensorArrays\n",
    "    max_time = ...  # this is the max time step per batch\n",
    "    inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_time, clear_after_read=False)\n",
    "    inputs_ta = inputs_ta.unstack(_transpose_batch_time(input_))  # model_input is the input placeholder\n",
    "    input_dim = input_.get_shape()[-1].value  # the dimensionality of the input to each time step\n",
    "    output_dim = ...  # the dimensionality of the model's output at each time step\n",
    "    \n",
    "    def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "        \"\"\"\n",
    "        Loop function that allows to control input to the rnn cell and manipulate cell outputs.\n",
    "        :param time: current time step\n",
    "        :param cell_output: output from previous time step or None if time == 0\n",
    "        :param cell_state: cell state from previous time step\n",
    "        :param loop_state: custom loop state to share information between different iterations of this loop fn\n",
    "        :return: tuple consisting of\n",
    "          elements_finished: tensor of size [bach_size] which is True for sequences that have reached their end,\n",
    "            needed because of variable sequence size\n",
    "          next_input: input to next time step\n",
    "          next_cell_state: cell state forwarded to next time step\n",
    "          emit_output: The first return argument of raw_rnn. This is not necessarily the output of the RNN cell,\n",
    "            but could e.g. be the output of a dense layer attached to the rnn layer.\n",
    "          next_loop_state: loop state forwarded to the next time step\n",
    "        \"\"\"\n",
    "        if cell_output is None:\n",
    "            # time == 0, used for initialization before first call to cell\n",
    "            next_cell_state = initial_state\n",
    "            # the emit_output in this case tells TF how future emits look\n",
    "            emit_output = tf.zeros([output_dim])\n",
    "        else:\n",
    "            # t > 0, called right after call to cell, i.e. cell_output is the output from time t-1.\n",
    "            # here you can do whatever ou want with cell_output before assigning it to emit_output.\n",
    "            # In this case, we don't do anything\n",
    "            next_cell_state = cell_state\n",
    "            emit_output = cell_output  \n",
    "\n",
    "        # check which elements are finished\n",
    "        elements_finished = (time >= seq_lengths)\n",
    "        finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "        # assemble cell input for upcoming time step\n",
    "        current_output = emit_output if cell_output is not None else None\n",
    "        input_original = inputs_ta.read(time)  # tensor of shape (None, input_dim)\n",
    "\n",
    "        if current_output is None:\n",
    "            # this is the initial step, i.e. there is no output from a previous time step, what we feed here\n",
    "            # can highly depend on the data. In this case we just assign the actual input in the first time step.\n",
    "            next_in = input_original\n",
    "        else:\n",
    "            # time > 0, so just use previous output as next input\n",
    "            # here you could do fancier things, whatever you want to do before passing the data into the rnn cell\n",
    "            # if here you were to pass input_original than you would get the normal behaviour of dynamic_rnn\n",
    "            next_in = current_output\n",
    "\n",
    "        next_input = tf.cond(finished,\n",
    "                             lambda: tf.zeros([self.batch_size, input_dim], dtype=tf.float32),  # copy through zeros\n",
    "                             lambda: next_in)  # if not finished, feed the previous output as next input\n",
    "\n",
    "        # set shape manually, otherwise it is not defined for the last dimensions\n",
    "        next_input.set_shape([None, input_dim])\n",
    "\n",
    "        # loop state not used in this example\n",
    "        next_loop_state = None\n",
    "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "    outputs_ta, last_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "    outputs = _transpose_batch_time(outputs_ta.stack())\n",
    "    final_state = last_state\n",
    "\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image size \n",
    "\n",
    "(288, 432, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(os.path.join('..', 'target', '0', '0.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_and_resize (frame):\n",
    "    \n",
    "    # cv2.resize(frame, (10, 10), interpolation = cv2.INTER_AREA)\n",
    "#     return frame[36:250, 114:328, :]\n",
    "    return cv2.resize(frame[36:250, 114:328, :], (15, 15), interpolation = cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "counter = 0\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    data.append(crop_and_resize(frame))\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-e3ca9b474a6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pl.imshow(data[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get instruction signal\n",
    "\n",
    "Parse the video file into instruction sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instruction_dict = {'north': 0, 'east': 1, 'south': 2, 'west': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_instruction_dict = dict((v,k) for (k,v) in instruction_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode ( sequence ):\n",
    "    return ','.join([reverse_instruction_dict[s] for s in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruction_btw_frame( prev_frame, cur_frame ):\n",
    "    diff = abs(prev_frame.astype(np.int32) - cur_frame.astype(np.int32))\n",
    "    vals = np.argwhere(diff >= 20)\n",
    "    \n",
    "    points = set()\n",
    "    for val in vals:\n",
    "        points.add(tuple(val[:2]))\n",
    "    \n",
    "    # if prev_frame[points[0]] ~ (255,255,255), points[0] is target, points[1] is start\n",
    "    # otherwise reverse\n",
    "    \n",
    "    points = list(points)\n",
    "    \n",
    "    if np.sum(abs(prev_frame[points[0]].astype(np.int32) - np.array([255, 255, 255]))) > 20:\n",
    "        source = np.array(points[0])\n",
    "        target = np.array(points[1])\n",
    "    else:\n",
    "        source = np.array(points[1])\n",
    "        target = np.array(points[0])\n",
    "\n",
    "        \n",
    "    if np.array_equal(target - source , [0, 1]):\n",
    "        return 1\n",
    "    if np.array_equal(target - source , [0, -1]):\n",
    "        return 3\n",
    "    if np.array_equal(target - source , [1, 0]):\n",
    "        return 2\n",
    "    if np.array_equal(target - source , [-1, 0]):\n",
    "        return 0\n",
    "        \n",
    "\n",
    "def get_instructions ( video_path, debug = True ):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    prev_frame = None\n",
    "    instructions = []\n",
    "\n",
    "    counter = 0\n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        c_frame = crop_and_resize(frame)\n",
    "        \n",
    "        # Get the instruction between prev_frame and frame\n",
    "        if prev_frame is not None:\n",
    "            instruction = get_instruction_btw_frame (prev_frame, c_frame)\n",
    "            instructions.append(instruction)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        prev_frame = c_frame\n",
    "        \n",
    "    #pl.imshow(prev_frame)\n",
    "    \n",
    "    if debug :\n",
    "        print ('There are %d frames ' % counter)\n",
    "        print ('The list of instructions are %s ' % decode( instructions ))\n",
    "    \n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 frames \n",
      "The list of instructions are south,south,south,south,south,south,west,west,south,south,west,west,west,west,west,west,south,west \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instructions(os.path.join('..', 'target', '0', '0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "LSTM model with attention with the image being additional input to encoded vector.\n",
    "\n",
    "The simplest model would have image flattened out into an 100-cell vectors. Colors are coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
