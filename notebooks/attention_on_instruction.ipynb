{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'attention_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d8be936b6260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_equal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mattention_decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'attention_decoder'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, LSTM, Dense, RepeatVector\n",
    "from numpy import array, array_equal, argmax\n",
    "import tensorflow as tf\n",
    "import attention_decoder\n",
    "\n",
    "\"\"\"\n",
    "Sample keras model\n",
    "\"\"\"\n",
    "def create_model_1():\n",
    "    Note = Input(shape=(sequence_length, n_vocab), dtype = 'float32')\n",
    "    y = LSTM(400, input_shape = (sequence_length, n_vocab), return_sequences = True ) (Note)\n",
    "    y = Dropout(0.4) (y)\n",
    "    y = LSTM(400) (y)\n",
    "    y = Dropout(0.4) (y)\n",
    "    # Two weights, two bias\n",
    "    # If coordinates of input is (X1, X2), and this layer is (Y1, Y2, Y3, Y4)\n",
    "    # Result would be \n",
    "    y1 = Lambda(lambda x: x*2)( Dense(2, activation = 'tanh') (y) )\n",
    "    y2 = Dense(2, activation = 'linear') (y)\n",
    "    Coordinates = Input(shape= (2, ), dtype = 'float32')\n",
    "    print (keras.backend.shape(y1))\n",
    "    print (keras.backend.shape(Coordinates))\n",
    "    c1 = Multiply()([y1, Coordinates])\n",
    "    c = Add()([y2, c1])\n",
    "    m = Model(inputs = [Note, Coordinates], outputs = c)\n",
    "\n",
    "    print (m.summary())\n",
    "    m.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return m\n",
    "\n",
    "def create_sample_attention_model () :\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "    model.add(attention_decoder.AttentionDecoder(150, n_features))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like for this problem, I need to use Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_ta = tf.TensorArray(size=784, dtype=tf.float32)  # store trained/sampled pixel\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None for time == 0\n",
    "    if cell_output is None:\n",
    "        # time=0, everything here will be used for initialization only\n",
    "        next_cell_state = cell_init_state\n",
    "        next_pixel = cell_init_pixel\n",
    "        next_loop_state = output_ta\n",
    "    else:  \n",
    "        # pass the last state to the next\n",
    "        next_cell_state = cell_state\n",
    "        next_pixel = tf.cond(is_training,\n",
    "                         lambda: inputs_ta.read(time - 1),\n",
    "                         lambda: tf.contrib.distributions.Bernoulli(\n",
    "                             probs=tf.nn.sigmoid(tf.layers.dense(cell_output, 1, \n",
    "                                 name='output_to_p', activation=tf.nn.sigmoid,\n",
    "                                 reuse=True)),\n",
    "                             dtype=tf.float32).sample())\n",
    "        next_loop_state = loop_state.write(time - 1, next_pixel)\n",
    "    elements_finished = (time >= 784)\n",
    "    return (elements_finished, next_pixel, next_cell_state,\n",
    "            emit_output, next_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "\n",
    "def sampling_rnn(self, cell, initial_state, input_, seq_lengths):\n",
    "\n",
    "    # raw_rnn expects time major inputs as TensorArrays\n",
    "    max_time = ...  # this is the max time step per batch\n",
    "    inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_time, clear_after_read=False)\n",
    "    inputs_ta = inputs_ta.unstack(_transpose_batch_time(input_))  # model_input is the input placeholder\n",
    "    input_dim = input_.get_shape()[-1].value  # the dimensionality of the input to each time step\n",
    "    output_dim = ...  # the dimensionality of the model's output at each time step\n",
    "    \n",
    "    def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "        \"\"\"\n",
    "        Loop function that allows to control input to the rnn cell and manipulate cell outputs.\n",
    "        :param time: current time step\n",
    "        :param cell_output: output from previous time step or None if time == 0\n",
    "        :param cell_state: cell state from previous time step\n",
    "        :param loop_state: custom loop state to share information between different iterations of this loop fn\n",
    "        :return: tuple consisting of\n",
    "          elements_finished: tensor of size [bach_size] which is True for sequences that have reached their end,\n",
    "            needed because of variable sequence size\n",
    "          next_input: input to next time step\n",
    "          next_cell_state: cell state forwarded to next time step\n",
    "          emit_output: The first return argument of raw_rnn. This is not necessarily the output of the RNN cell,\n",
    "            but could e.g. be the output of a dense layer attached to the rnn layer.\n",
    "          next_loop_state: loop state forwarded to the next time step\n",
    "        \"\"\"\n",
    "        if cell_output is None:\n",
    "            # time == 0, used for initialization before first call to cell\n",
    "            next_cell_state = initial_state\n",
    "            # the emit_output in this case tells TF how future emits look\n",
    "            emit_output = tf.zeros([output_dim])\n",
    "        else:\n",
    "            # t > 0, called right after call to cell, i.e. cell_output is the output from time t-1.\n",
    "            # here you can do whatever ou want with cell_output before assigning it to emit_output.\n",
    "            # In this case, we don't do anything\n",
    "            next_cell_state = cell_state\n",
    "            emit_output = cell_output  \n",
    "\n",
    "        # check which elements are finished\n",
    "        elements_finished = (time >= seq_lengths)\n",
    "        finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "        # assemble cell input for upcoming time step\n",
    "        current_output = emit_output if cell_output is not None else None\n",
    "        input_original = inputs_ta.read(time)  # tensor of shape (None, input_dim)\n",
    "\n",
    "        if current_output is None:\n",
    "            # this is the initial step, i.e. there is no output from a previous time step, what we feed here\n",
    "            # can highly depend on the data. In this case we just assign the actual input in the first time step.\n",
    "            next_in = input_original\n",
    "        else:\n",
    "            # time > 0, so just use previous output as next input\n",
    "            # here you could do fancier things, whatever you want to do before passing the data into the rnn cell\n",
    "            # if here you were to pass input_original than you would get the normal behaviour of dynamic_rnn\n",
    "            next_in = current_output\n",
    "\n",
    "        next_input = tf.cond(finished,\n",
    "                             lambda: tf.zeros([self.batch_size, input_dim], dtype=tf.float32),  # copy through zeros\n",
    "                             lambda: next_in)  # if not finished, feed the previous output as next input\n",
    "\n",
    "        # set shape manually, otherwise it is not defined for the last dimensions\n",
    "        next_input.set_shape([None, input_dim])\n",
    "\n",
    "        # loop state not used in this example\n",
    "        next_loop_state = None\n",
    "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "    outputs_ta, last_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "    outputs = _transpose_batch_time(outputs_ta.stack())\n",
    "    final_state = last_state\n",
    "\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image size \n",
    "\n",
    "(288, 432, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(os.path.join('..', 'target', '0', '0.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_and_resize (frame):\n",
    "    \n",
    "    # cv2.resize(frame, (10, 10), interpolation = cv2.INTER_AREA)\n",
    "#     return frame[36:250, 114:328, :]\n",
    "    return cv2.resize(frame[36:250, 114:328, :], (15, 15), interpolation = cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "counter = 0\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    data.append(crop_and_resize(frame))\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-e3ca9b474a6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pl.imshow(data[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get instruction signal\n",
    "\n",
    "Parse the video file into instruction sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instruction_dict = {'up': 0, 'right': 1, 'down': 2, 'left': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_instruction_dict = dict((v,k) for (k,v) in instruction_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode ( sequence ):\n",
    "    return ' '.join([reverse_instruction_dict[s] for s in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_instruction_btw_frame( prev_frame, cur_frame ):\n",
    "    diff = abs(prev_frame.astype(np.int32) - cur_frame.astype(np.int32))\n",
    "    vals = np.argwhere(diff >= 20)\n",
    "    \n",
    "    points = set()\n",
    "    for val in vals:\n",
    "        points.add(tuple(val[:2]))\n",
    "    \n",
    "    # if prev_frame[points[0]] ~ (255,255,255), points[0] is target, points[1] is start\n",
    "    # otherwise reverse\n",
    "    \n",
    "    points = list(points)\n",
    "    \n",
    "    if np.sum(abs(prev_frame[points[0]].astype(np.int32) - np.array([255, 255, 255]))) > 20:\n",
    "        source = np.array(points[0])\n",
    "        target = np.array(points[1])\n",
    "    else:\n",
    "        source = np.array(points[1])\n",
    "        target = np.array(points[0])\n",
    "\n",
    "        \n",
    "    if np.array_equal(target - source , [0, 1]):\n",
    "        return 1\n",
    "    if np.array_equal(target - source , [0, -1]):\n",
    "        return 3\n",
    "    if np.array_equal(target - source , [1, 0]):\n",
    "        return 2\n",
    "    if np.array_equal(target - source , [-1, 0]):\n",
    "        return 0\n",
    "        \n",
    "\n",
    "def get_instructions ( video_path, debug = True ):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    prev_frame = None\n",
    "    instructions = []\n",
    "\n",
    "    counter = 0\n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        c_frame = crop_and_resize(frame)\n",
    "        \n",
    "        # Get the instruction between prev_frame and frame\n",
    "        if prev_frame is not None:\n",
    "            instruction = get_instruction_btw_frame (prev_frame, c_frame)\n",
    "            instructions.append(instruction)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        prev_frame = c_frame\n",
    "        \n",
    "    #pl.imshow(prev_frame)\n",
    "    \n",
    "    if debug :\n",
    "        print ('There are %d frames ' % counter)\n",
    "        print ('The list of instructions are %s ' % decode( instructions ))\n",
    "    \n",
    "    return decode( instructions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 frames \n",
      "The list of instructions are down,down,down,down,down,down,left,left,down,down,left,left,left,left,left,left,down,left \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instructions(os.path.join('..', 'target', '0', '0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "LSTM model with attention with the image being additional input to encoded vector.\n",
    "\n",
    "The simplest model would have image flattened out into an 100-cell vectors. Colors are coded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate instructions and commands text files and vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_file = 'instructions.txt'\n",
    "tgt_file = 'commands.txt'\n",
    "src_vocab_file = 'instructions.vocab'\n",
    "tgt_vocab_file = 'commands.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample text\n",
    "Alexander Reese,100,move the purple block between the yellow block and elastic red block stop in the creak of the purple L-shape block\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "puzzle_to_instructions = defaultdict(list)\n",
    "\n",
    "with open(os.path.join('..', 'annotation.csv'), 'r') as fh:\n",
    "    for line in fh:\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        puzzle = int(parts[1])\n",
    "        text = ' '.join( parts[2:] )\n",
    "        \n",
    "        text = text.replace ('-', ' ').strip()\n",
    "        \n",
    "        if text != '':\n",
    "            puzzle_to_instructions[puzzle].append(text)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('puzzle.dat', 'wb') as fh:\n",
    "    pickle.dump(puzzle_to_instructions, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', 'data', src_file), 'w') as fh:\n",
    "    with open(os.path.join('..', 'data', tgt_file), 'w') as fh2:\n",
    "        \n",
    "        for puzzle in range(200):\n",
    "            instructions = puzzle_to_instructions[puzzle]\n",
    "            \n",
    "            directory = puzzle // 100\n",
    "            \n",
    "            commands = get_instructions(os.path.join('..', 'target', str(directory), str(puzzle) + '.mp4'), debug = False)\n",
    "            \n",
    "            for instruction in instructions[:-1]:\n",
    "                fh.write(instruction.lower())\n",
    "                fh.write('\\n')\n",
    "                fh2.write(commands)\n",
    "                fh2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import VocabularyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VocabularyProcessor(min_frequency=10, max_document_length = 100)\n",
    "\n",
    "vp.fit(t for t in puzzle_to_instructions[puzzle] for puzzle in range(200)) \n",
    "\n",
    "t = vp.vocabulary_\n",
    "\n",
    "with open(os.path.join('..', 'data', src_vocab_file), 'w') as fh:\n",
    "    for word in t._reverse_mapping:\n",
    "        fh.write(word)\n",
    "        fh.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create eval files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_src_file = 'eval_instructions.txt'\n",
    "eval_tgt_file = 'eval_commands.txt'\n",
    "\n",
    "with open(os.path.join('..', 'data', eval_src_file), 'w') as fh:\n",
    "    with open(os.path.join('..', 'data', eval_tgt_file), 'w') as fh2:\n",
    "        \n",
    "        for puzzle in range(200):\n",
    "            instructions = puzzle_to_instructions[puzzle]\n",
    "            \n",
    "            directory = puzzle // 100\n",
    "            \n",
    "            commands = get_instructions(os.path.join('..', 'target', str(directory), str(puzzle) + '.mp4'), debug = False)\n",
    "            \n",
    "            instruction = instructions[-1]\n",
    "            fh.write(instruction.lower())\n",
    "            fh.write('\\n')\n",
    "            fh2.write(commands)\n",
    "            fh2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result from one run with\n",
    "\n",
    "```\n",
    "num_units=128,\n",
    "optimizer=\"sgd\",\n",
    "learning_rate=0.2,\n",
    "num_train_steps=2000,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ppls = [(50, 3.622860993755861), (100, 2.861048874787788), (150, 2.7144358871103695), (200, 2.6244221451141554), (250, 2.559963936057708), (300, 2.4906299530576717), (350, 2.4611581223296883), (400, 2.4081972736561292), (450, 2.376238414347918), (500, 2.3149748093621305), (550, 2.2807155686298373), (600, 2.2271950330723733), (650, 2.1852261228233196), (700, 2.133554434277888), (750, 2.0864403448274174), (800, 2.0275573143614056), (850, 1.966460910555963), (900, 1.9100762230672461), (950, 1.8597766285396615), (1000, 1.8106065211986038), (1050, 1.7577774811347908), (1100, 1.7159142948821353), (1150, 1.6667710320040563), (1200, 1.6176787509841954), (1250, 1.461931473293547), (1300, 1.4199632900484251), (1350, 1.395266490093124), (1400, 1.3680164569824662), (1450, 1.3074188319379587), (1500, 1.289243310363728), (1550, 1.2762814642136484), (1600, 1.2600097457796327), (1650, 1.242998112668705), (1700, 1.2327585428902734), (1750, 1.222338292542516), (1800, 1.2157578290950826), (1850, 1.2029619735575519), (1900, 1.1996501731730116), (1950, 1.1976915749849835), (2000, 1.196468185521777)]\n",
    "dev_ppls = [(200, 2.57433564083528), (400, 2.456461232812054), (600, 2.2005683089711163), (800, 2.0124692521350296), (1000, 1.814219438942708), (1200, 1.723100523239358), (1400, 1.6296202548584209), (1600, 1.6325943888569625), (1800, 1.6608891188844597), (2000, 1.673063526619738)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20285bc6780>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20285bc6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([t[0] for t in train_ppls], [t[1] for t in train_ppls])\n",
    "plt.plot([t[0] for t in dev_ppls], [t[1] for t in dev_ppls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
